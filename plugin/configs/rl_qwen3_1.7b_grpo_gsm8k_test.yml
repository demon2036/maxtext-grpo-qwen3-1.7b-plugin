# Plugin RL config: Qwen3-1.7B GRPO on GSM8K (10-step smoke test)
#
# - Inherits upstream MaxText RL config.
# - NOTE: Upstream MaxText restricts `model_name` to a fixed allowlist, and Tunix
#   requires `model_name` to be present in `HF_MODEL_CONFIGS` for vLLM weight mapping.
#   We therefore set `model_name=qwen3-0.6b` (allowlisted) but override its model
#   config locally under `plugin/configs/models/qwen3-0.6b.yml` to match Qwen3-1.7B.
#
# NOTE: Pass `run_name=... base_output_directory=...` via CLI.

base_config: "../../third_party/maxtext/src/MaxText/configs/rl.yml"

# --- Model / Tokenizer ---
model_name: "qwen3-0.6b"
tokenizer_path: "Qwen/Qwen3-1.7B"
tokenizer_type: "huggingface"

# --- Tunix (PyPI) compatibility ---
# Tunix 0.1.5 expects a mesh axis named "model" for vLLM tensor parallelism.
# MaxText defaults to "tensor". Add a no-op "model" axis (size=1 here) so both
# conventions work without patching upstream MaxText or Tunix internals.
mesh_axes:
  - data
  - stage
  - fsdp
  - fsdp_transpose
  - sequence
  - context
  - context_autoregressive
  - model
  - tensor
  - tensor_transpose
  - tensor_sequence
  - expert
  - autoregressive

# --- Smoke-test training length (10 steps) ---
num_batches: 10
num_iterations: 1
train_fraction: 1.0
num_epoch: 1

# --- Faster/safer generation settings (smoke test) ---
max_prefill_predict_length: 128
max_target_length: 512
kv_cache_buffer: 64

# Keep eval short
num_test_batches: 2
num_eval_passes: 1

# Reduce noise
debug:
  rl: False
